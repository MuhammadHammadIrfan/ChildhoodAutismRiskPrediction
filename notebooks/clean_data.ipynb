{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23d501d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA CLEANING PROCESS\n",
      "============================================================\n",
      "\n",
      "Original dataset shape: 292 rows √ó 21 columns\n",
      "\n",
      "Missing values BEFORE cleaning:\n",
      "  age: 4 missing values\n",
      "  ethnicity: 43 missing values\n",
      "  relation: 43 missing values\n",
      "\n",
      "------------------------------------------------------------\n",
      "APPLYING CLEANING RULES:\n",
      "------------------------------------------------------------\n",
      "‚úì Filled 4 missing 'age' values with mode: 4.0\n",
      "‚úì Filled 43 missing 'ethnicity' values with 'unknown'\n",
      "‚úì Standardized 1 'self' values to 'Self' in 'relation' column\n",
      "‚úì Filled 43 missing 'relation' values with 'Parent'\n",
      "\n",
      "------------------------------------------------------------\n",
      "Missing values AFTER cleaning:\n",
      "  No missing values remaining!\n",
      "\n",
      "============================================================\n",
      "‚úì Cleaned data saved to: d:\\Labs\\ML\\ML Project\\ChildhoodAutismRiskPrediction\\data\\clean\\autism_screening_cleaned.csv\n",
      "============================================================\n",
      "\n",
      "SUMMARY:\n",
      "  Total rows: 292\n",
      "  Total columns: 21\n",
      "  Unique values in 'relation' column: ['Parent' 'Self' 'Relative' \"'Health care professional'\"]\n",
      "  Value counts for 'relation':\n",
      "relation\n",
      "Parent                        257\n",
      "Relative                       17\n",
      "'Health care professional'     13\n",
      "Self                            5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muham\\AppData\\Local\\Temp\\ipykernel_668\\702004498.py:36: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['age'].fillna(age_mode, inplace=True)\n",
      "C:\\Users\\muham\\AppData\\Local\\Temp\\ipykernel_668\\702004498.py:44: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['ethnicity'].fillna('unknown', inplace=True)\n",
      "C:\\Users\\muham\\AppData\\Local\\Temp\\ipykernel_668\\702004498.py:62: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['relation'].fillna('Parent', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def clean_missing_values(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Clean the dataset by filling missing values according to specified rules.\n",
    "    \n",
    "    Parameters:\n",
    "    input_path (str): Path to the raw CSV file\n",
    "    output_path (str): Path to save the cleaned CSV file\n",
    "    \"\"\"\n",
    "    # Read the raw data\n",
    "    df = pd.read_csv(input_path)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"DATA CLEANING PROCESS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nOriginal dataset shape: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
    "    \n",
    "    # Display missing values before cleaning\n",
    "    print(\"\\nMissing values BEFORE cleaning:\")\n",
    "    missing_before = df.isnull().sum()\n",
    "    missing_cols = missing_before[missing_before > 0]\n",
    "    if len(missing_cols) > 0:\n",
    "        for col, count in missing_cols.items():\n",
    "            print(f\"  {col}: {count} missing values\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"APPLYING CLEANING RULES:\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    # 1. Fill missing age values with mode\n",
    "    if df['age'].isnull().sum() > 0:\n",
    "        age_mode = df['age'].mode()[0]\n",
    "        missing_age_count = df['age'].isnull().sum()\n",
    "        df['age'].fillna(age_mode, inplace=True)\n",
    "        print(f\"‚úì Filled {missing_age_count} missing 'age' values with mode: {age_mode}\")\n",
    "    else:\n",
    "        print(\"‚úì No missing values in 'age' column\")\n",
    "    \n",
    "    # 2. Fill missing ethnicity values with \"unknown\"\n",
    "    if df['ethnicity'].isnull().sum() > 0:\n",
    "        missing_ethnicity_count = df['ethnicity'].isnull().sum()\n",
    "        df['ethnicity'].fillna('unknown', inplace=True)\n",
    "        print(f\"‚úì Filled {missing_ethnicity_count} missing 'ethnicity' values with 'unknown'\")\n",
    "    else:\n",
    "        print(\"‚úì No missing values in 'ethnicity' column\")\n",
    "    \n",
    "    # 3. Standardize relation column: convert \"self\" to \"Self\"\n",
    "    if 'relation' in df.columns:\n",
    "        # Count occurrences before standardization\n",
    "        self_lowercase_count = (df['relation'] == 'self').sum()\n",
    "        if self_lowercase_count > 0:\n",
    "            df['relation'] = df['relation'].replace('self', 'Self')\n",
    "            print(f\"‚úì Standardized {self_lowercase_count} 'self' values to 'Self' in 'relation' column\")\n",
    "        else:\n",
    "            print(\"‚úì No 'self' values to standardize in 'relation' column\")\n",
    "    \n",
    "    # 4. Fill missing relation values with \"Parent\"\n",
    "    if df['relation'].isnull().sum() > 0:\n",
    "        missing_relation_count = df['relation'].isnull().sum()\n",
    "        df['relation'].fillna('Parent', inplace=True)\n",
    "        print(f\"‚úì Filled {missing_relation_count} missing 'relation' values with 'Parent'\")\n",
    "    else:\n",
    "        print(\"‚úì No missing values in 'relation' column\")\n",
    "    \n",
    "    # Display missing values after cleaning\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"Missing values AFTER cleaning:\")\n",
    "    missing_after = df.isnull().sum()\n",
    "    missing_cols_after = missing_after[missing_after > 0]\n",
    "    if len(missing_cols_after) > 0:\n",
    "        for col, count in missing_cols_after.items():\n",
    "            print(f\"  {col}: {count} missing values\")\n",
    "    else:\n",
    "        print(\"  No missing values remaining!\")\n",
    "    \n",
    "    # Save cleaned data\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚úì Cleaned data saved to: {output_path}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Display summary statistics\n",
    "    print(\"SUMMARY:\")\n",
    "    print(f\"  Total rows: {df.shape[0]}\")\n",
    "    print(f\"  Total columns: {df.shape[1]}\")\n",
    "    print(f\"  Unique values in 'relation' column: {df['relation'].unique()}\")\n",
    "    print(f\"  Value counts for 'relation':\")\n",
    "    print(df['relation'].value_counts().to_string())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Define paths for notebook environment\n",
    "# Navigate up from notebooks/hammad/ to project root\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "input_file = os.path.join(project_root, 'data', 'raw', 'autism_screening.csv')\n",
    "output_file = os.path.join(project_root, 'data', 'clean', 'autism_screening_cleaned.csv')\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "# Run the cleaning process\n",
    "# Close any open file handles to the output file first\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "cleaned_df = clean_missing_values(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d584e3",
   "metadata": {},
   "source": [
    "# Additional Data Cleaning\n",
    "Now we'll fix additional issues:\n",
    "1. Replace \"unknown\" with \"Other\" in ethnicity (since \"Other\" already exists)\n",
    "2. Remove single quotes around values in ethnicity, country_of_res, and age_desc\n",
    "3. Strip leading/trailing spaces from all columns\n",
    "4. Display unique values for all columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de85ec9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ADDITIONAL DATA CLEANING\n",
      "============================================================\n",
      "\n",
      "Dataset shape: 292 rows √ó 21 columns\n",
      "\n",
      "‚úì Replaced 43 'unknown' values with 'Others' in ethnicity column\n",
      "‚úì Removed single quotes from 'ethnicity' column\n",
      "‚úì Removed single quotes from 'country_of_res' column\n",
      "‚úì Removed single quotes from 'age_desc' column\n",
      "\n",
      "------------------------------------------------------------\n",
      "Removing leading/trailing spaces from all columns...\n",
      "------------------------------------------------------------\n",
      "  ‚úì Cleaned 27 values in 'ethnicity' column\n",
      "\n",
      "============================================================\n",
      "SAVING UPDATED CLEANED DATA\n",
      "============================================================\n",
      "‚úì Updated cleaned data saved to: d:\\Labs\\ML\\ML Project\\ChildhoodAutismRiskPrediction\\data\\clean\\autism_screening_cleaned.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the cleaned data\n",
    "df = pd.read_csv(output_file)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ADDITIONAL DATA CLEANING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDataset shape: {df.shape[0]} rows √ó {df.shape[1]} columns\\n\")\n",
    "\n",
    "# 1. Replace \"unknown\" with \"Other\" in ethnicity column\n",
    "if 'unknown' in df['ethnicity'].values:\n",
    "    unknown_count = (df['ethnicity'] == 'unknown').sum()\n",
    "    df['ethnicity'] = df['ethnicity'].replace('unknown', 'Others')\n",
    "    print(f\"‚úì Replaced {unknown_count} 'unknown' values with 'Others' in ethnicity column\")\n",
    "else:\n",
    "    print(\"‚úì No 'unknown' values found in ethnicity column\")\n",
    "\n",
    "# 2. Remove single quotes from specific columns\n",
    "columns_to_clean = ['ethnicity', 'country_of_res', 'age_desc']\n",
    "\n",
    "for col in columns_to_clean:\n",
    "    if col in df.columns:\n",
    "        # Remove leading and trailing single quotes\n",
    "        df[col] = df[col].astype(str).str.strip(\"'\")\n",
    "        print(f\"‚úì Removed single quotes from '{col}' column\")\n",
    "\n",
    "# 3. Strip leading and trailing spaces from all columns\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Removing leading/trailing spaces from all columns...\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':  # Only process string columns\n",
    "        # Count rows with leading/trailing spaces\n",
    "        spaces_before = df[col].astype(str).str.len()\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "        spaces_after = df[col].astype(str).str.len()\n",
    "        cleaned = (spaces_before != spaces_after).sum()\n",
    "        \n",
    "        if cleaned > 0:\n",
    "            print(f\"  ‚úì Cleaned {cleaned} values in '{col}' column\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING UPDATED CLEANED DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save the updated cleaned data\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"‚úì Updated cleaned data saved to: {output_file}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22287f47",
   "metadata": {},
   "source": [
    "# Display Unique Values for All Columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf360f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "UNIQUE VALUES FOR ALL COLUMNS\n",
      "================================================================================\n",
      "\n",
      "\n",
      "A1_SCORE (2 unique values):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "- 0\n",
      "- 1\n",
      "\n",
      "A2_SCORE (2 unique values):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "- 0\n",
      "- 1\n",
      "\n",
      "A3_SCORE (2 unique values):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "- 0\n",
      "- 1\n",
      "\n",
      "A4_SCORE (2 unique values):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "- 0\n",
      "- 1\n",
      "\n",
      "A5_SCORE (2 unique values):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "- 0\n",
      "- 1\n",
      "\n",
      "A6_SCORE (2 unique values):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "- 0\n",
      "- 1\n",
      "\n",
      "A7_SCORE (2 unique values):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "- 0\n",
      "- 1\n",
      "\n",
      "A8_SCORE (2 unique values):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "- 0\n",
      "- 1\n",
      "\n",
      "A9_SCORE (2 unique values):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "- 0\n",
      "- 1\n",
      "\n",
      "A10_SCORE (2 unique values):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "- 0\n",
      "- 1\n",
      "\n",
      "AGE (8 unique values):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "- 10.0\n",
      "- 11.0\n",
      "- 4.0\n",
      "- 5.0\n",
      "- 6.0\n",
      "- 7.0\n",
      "- 8.0\n",
      "- 9.0\n",
      "\n",
      "GENDER (2 unique values):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "- f\n",
      "- m\n",
      "\n",
      "Value counts:\n",
      "\n",
      "m: 208\n",
      "f: 84\n",
      "\n",
      "ETHNICITY (10 unique values):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "- Asian\n",
      "- Black\n",
      "- Hispanic\n",
      "- Latino\n",
      "- Middle Eastern\n",
      "- Others\n",
      "- Pasifika\n",
      "- South Asian\n",
      "- Turkish\n",
      "- White-European\n",
      "\n",
      "Value counts:\n",
      "\n",
      "White-European: 108\n",
      "Others: 57\n",
      "Asian: 46\n",
      "Middle Eastern: 27\n",
      "South Asian: 21\n",
      "Black: 14\n",
      "Latino: 8\n",
      "Hispanic: 7\n",
      "Pasifika: 2\n",
      "Turkish: 2\n",
      "\n",
      "JAUNDICE (2 unique values):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "- no\n",
      "- yes\n",
      "\n",
      "Value counts:\n",
      "\n",
      "no: 212\n",
      "yes: 80\n",
      "\n",
      "AUTISM (2 unique values):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "- no\n",
      "- yes\n",
      "\n",
      "Value counts:\n",
      "\n",
      "no: 243\n",
      "yes: 49\n",
      "\n",
      "COUNTRY_OF_RES (52 unique values):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Afghanistan\n",
      "2. Argentina\n",
      "3. Armenia\n",
      "4. Australia\n",
      "5. Austria\n",
      "6. Bahrain\n",
      "7. Bangladesh\n",
      "8. Bhutan\n",
      "9. Brazil\n",
      "10. Bulgaria\n",
      "11. Canada\n",
      "12. China\n",
      "13. Costa Rica\n",
      "14. Egypt\n",
      "15. Europe\n",
      "16. Georgia\n",
      "17. Germany\n",
      "18. Ghana\n",
      "19. India\n",
      "20. Iraq\n",
      "21. Ireland\n",
      "22. Isle of Man\n",
      "23. Italy\n",
      "24. Japan\n",
      "25. Jordan\n",
      "26. Kuwait\n",
      "27. Latvia\n",
      "28. Lebanon\n",
      "29. Libya\n",
      "30. Malaysia\n",
      "31. Malta\n",
      "32. Mexico\n",
      "33. Nepal\n",
      "34. Netherlands\n",
      "35. New Zealand\n",
      "36. Nigeria\n",
      "37. Oman\n",
      "38. Pakistan\n",
      "39. Philippines\n",
      "40. Qatar\n",
      "41. Romania\n",
      "42. Russia\n",
      "43. Saudi Arabia\n",
      "44. South Africa\n",
      "45. South Korea\n",
      "46. Sweden\n",
      "47. Syria\n",
      "48. Turkey\n",
      "49. U.S. Outlying Islands\n",
      "50. United Arab Emirates\n",
      "51. United Kingdom\n",
      "52. United States\n",
      "\n",
      "Value counts:\n",
      "\n",
      "United Kingdom: 49\n",
      "United States: 42\n",
      "India: 42\n",
      "Australia: 23\n",
      "Jordan: 20\n",
      "New Zealand: 13\n",
      "Egypt: 9\n",
      "United Arab Emirates: 7\n",
      "Canada: 7\n",
      "Bangladesh: 6\n",
      "Saudi Arabia: 4\n",
      "Philippines: 4\n",
      "Pakistan: 4\n",
      "Libya: 3\n",
      "Qatar: 3\n",
      "Armenia: 3\n",
      "Russia: 3\n",
      "Iraq: 3\n",
      "Syria: 3\n",
      "Bahrain: 2\n",
      "Malaysia: 2\n",
      "Italy: 2\n",
      "Brazil: 2\n",
      "Mexico: 2\n",
      "Afghanistan: 2\n",
      "Georgia: 2\n",
      "Austria: 2\n",
      "South Africa: 2\n",
      "Lebanon: 2\n",
      "Turkey: 2\n",
      "Malta: 1\n",
      "Bulgaria: 1\n",
      "Kuwait: 1\n",
      "Europe: 1\n",
      "Oman: 1\n",
      "Costa Rica: 1\n",
      "Ireland: 1\n",
      "Japan: 1\n",
      "Sweden: 1\n",
      "Argentina: 1\n",
      "South Korea: 1\n",
      "Netherlands: 1\n",
      "Germany: 1\n",
      "Romania: 1\n",
      "China: 1\n",
      "Latvia: 1\n",
      "U.S. Outlying Islands: 1\n",
      "Nigeria: 1\n",
      "Nepal: 1\n",
      "Isle of Man: 1\n",
      "Ghana: 1\n",
      "Bhutan: 1\n",
      "\n",
      "USED_APP_BEFORE (2 unique values):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "- no\n",
      "- yes\n",
      "\n",
      "Value counts:\n",
      "\n",
      "no: 281\n",
      "yes: 11\n",
      "\n",
      "RESULT (11 unique values):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "- 0\n",
      "- 1\n",
      "- 10\n",
      "- 2\n",
      "- 3\n",
      "- 4\n",
      "- 5\n",
      "- 6\n",
      "- 7\n",
      "- 8\n",
      "- 9\n",
      "\n",
      "AGE_DESC (1 unique values):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "- 4-11 years\n",
      "\n",
      "Value counts:\n",
      "\n",
      "4-11 years: 292\n",
      "\n",
      "RELATION (4 unique values):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "- 'Health care professional'\n",
      "- Parent\n",
      "- Relative\n",
      "- Self\n",
      "\n",
      "Value counts:\n",
      "\n",
      "Parent: 257\n",
      "Relative: 17\n",
      "'Health care professional': 13\n",
      "Self: 5\n",
      "\n",
      "CLASS (2 unique values):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "- NO\n",
      "- YES\n",
      "\n",
      "Value counts:\n",
      "\n",
      "NO: 151\n",
      "YES: 141\n",
      "\n",
      "================================================================================\n",
      "DATA CLEANING COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "\n",
      "‚úì Full report also saved to: d:\\Labs\\ML\\ML Project\\ChildhoodAutismRiskPrediction\\reports\\unique_values_report.txt\n",
      "‚úì Total columns processed: 21\n",
      "‚úì Columns: A1_Score, A2_Score, A3_Score, A4_Score, A5_Score, A6_Score, A7_Score, A8_Score, A9_Score, A10_Score, age, gender, ethnicity, jaundice, autism, country_of_res, used_app_before, result, age_desc, relation, class\n"
     ]
    }
   ],
   "source": [
    "# Set pandas display options to show all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Also save the report to a file to ensure nothing is truncated\n",
    "report_file = os.path.join(project_root, 'reports', 'unique_values_report.txt')\n",
    "os.makedirs(os.path.dirname(report_file), exist_ok=True)\n",
    "\n",
    "# Open file for writing\n",
    "with open(report_file, 'w', encoding='utf-8') as f:\n",
    "    # Display unique values for all columns\n",
    "    header = \"=\"*80 + \"\\nUNIQUE VALUES FOR ALL COLUMNS\\n\" + \"=\"*80 + \"\\n\"\n",
    "    print(header)\n",
    "    f.write(header)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        unique_vals = df[col].unique()\n",
    "        n_unique = len(unique_vals)\n",
    "        \n",
    "        section_header = f\"\\n{col.upper()} ({n_unique} unique values):\\n\" + \"-\" * 80 + \"\\n\"\n",
    "        print(section_header)\n",
    "        f.write(section_header)\n",
    "        \n",
    "        # Display all unique values\n",
    "        sorted_vals = sorted([str(val) for val in unique_vals])\n",
    "        \n",
    "        # Print each unique value\n",
    "        if n_unique > 30:\n",
    "            for i, val in enumerate(sorted_vals, 1):\n",
    "                line = f\"  {i}. {val}\\n\"\n",
    "                print(line.strip())\n",
    "                f.write(line)\n",
    "        else:\n",
    "            for val in sorted_vals:\n",
    "                line = f\"  - {val}\\n\"\n",
    "                print(line.strip())\n",
    "                f.write(line)\n",
    "        \n",
    "        # Show value counts for categorical columns\n",
    "        if n_unique <= 60 and df[col].dtype == 'object':\n",
    "            vc_header = f\"\\nValue counts:\\n\"\n",
    "            print(vc_header)\n",
    "            f.write(vc_header)\n",
    "            \n",
    "            value_counts = df[col].value_counts()\n",
    "            for val, count in value_counts.items():\n",
    "                line = f\"  {val}: {count}\\n\"\n",
    "                print(line.strip())\n",
    "                f.write(line)\n",
    "    \n",
    "    footer = \"\\n\" + \"=\"*80 + \"\\nDATA CLEANING COMPLETE!\\n\" + \"=\"*80 + \"\\n\"\n",
    "    print(footer)\n",
    "    f.write(footer)\n",
    "\n",
    "print(f\"\\n‚úì Full report also saved to: {report_file}\")\n",
    "print(f\"‚úì Total columns processed: {len(df.columns)}\")\n",
    "print(f\"‚úì Columns: {', '.join(df.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f24d77b",
   "metadata": {},
   "source": [
    "# Feature Encoding\n",
    "\n",
    "Now we'll encode categorical features for ML model compatibility:\n",
    "\n",
    "## Encoding Strategy:\n",
    "\n",
    "1. **Binary Encoding** (for 2-category columns):\n",
    "   - gender, jaundice, autism, used_app_before, class\n",
    "   - Simple 0/1 encoding (efficient for binary features)\n",
    "\n",
    "2. **Grouped + One-Hot Encoding** (for ethnicity):\n",
    "   - Keep categories with count >= 10: White-European (108), Others (57), Asian (46), Middle Eastern (27), South Asian (21), Black (14)\n",
    "   - Merge rare categories (Latino, Hispanic, Pasifika, Turkish) into \"Others\"\n",
    "   - Result: 6 categories ‚Üí 6 dummy variables\n",
    "   - **Why?** Reduces noise from rare categories and prevents overfitting\n",
    "\n",
    "3. **One-Hot Encoding** (for relation):\n",
    "   - relation (4 categories)\n",
    "   - **Best for:** Nominal categories with no ordinal relationship\n",
    "\n",
    "4. **Top-N + Other Encoding** (for high-cardinality column):\n",
    "   - country_of_res: Keep top 5, group rest as \"Other\"\n",
    "   - **Why?** 52 categories with only 292 rows would create sparse matrix\n",
    "   - **Rule of thumb:** When categories > 10% of samples, consider grouping\n",
    "\n",
    "## Columns to DROP:\n",
    "\n",
    "### ‚ö†Ô∏è Data Leakage - MUST DROP:\n",
    "- **result**: Sum of A1-A10 scores. If kept, model will cheat using this instead of learning from actual features!\n",
    "\n",
    "### No Variance - Should DROP:\n",
    "- **age_desc**: Only has 1 unique value ('4-11 years') - provides no information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6efee53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURE ENCODING PROCESS\n",
      "================================================================================\n",
      "\n",
      "Original shape: (292, 21)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1. BINARY ENCODING (2-category columns)\n",
      "--------------------------------------------------------------------------------\n",
      "  ‚úì gender: {'m': 1, 'f': 0}\n",
      "  ‚úì jaundice: {'yes': 1, 'no': 0}\n",
      "  ‚úì autism: {'yes': 1, 'no': 0}\n",
      "  ‚úì used_app_before: {'yes': 1, 'no': 0}\n",
      "  ‚úì class: {'YES': 1, 'NO': 0}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2. TOP-5 COUNTRY ENCODING\n",
      "--------------------------------------------------------------------------------\n",
      "  Top 5 countries: ['United Kingdom', 'United States', 'India', 'Australia', 'Jordan']\n",
      "\n",
      "  Grouped distribution:\n",
      "    Other: 116\n",
      "    United Kingdom: 49\n",
      "    India: 42\n",
      "    United States: 42\n",
      "    Australia: 23\n",
      "    Jordan: 20\n",
      "\n",
      "  ‚úì Created 6 country dummy variables\n",
      "  ‚úì Columns: ['country_Australia', 'country_India', 'country_Jordan', 'country_Other', 'country_United Kingdom', 'country_United States']\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3. GROUPED ETHNICITY ENCODING (keep categories with count >= 10)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  Original ethnicity distribution:\n",
      "    White-European: 108\n",
      "    Others: 57\n",
      "    Asian: 46\n",
      "    Middle Eastern: 27\n",
      "    South Asian: 21\n",
      "    Black: 14\n",
      "    Latino: 8\n",
      "    Hispanic: 7\n",
      "    Pasifika: 2\n",
      "    Turkish: 2\n",
      "\n",
      "  Keeping categories with count >= 10: ['White-European', 'Others', 'Asian', 'Middle Eastern', 'South Asian', 'Black']\n",
      "\n",
      "  Grouped ethnicity distribution:\n",
      "    White-European: 108\n",
      "    Others: 76\n",
      "    Asian: 46\n",
      "    Middle Eastern: 27\n",
      "    South Asian: 21\n",
      "    Black: 14\n",
      "\n",
      "  ‚úì ethnicity: 6 grouped categories ‚Üí 6 dummy variables\n",
      "  Columns: ['ethnicity_Asian', 'ethnicity_Black', 'ethnicity_Middle Eastern', 'ethnicity_Others', 'ethnicity_South Asian', 'ethnicity_White-European']\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "4. ONE-HOT ENCODING (relation column)\n",
      "--------------------------------------------------------------------------------\n",
      "  ‚úì Cleaned 'relation' column (removed quotes)\n",
      "  ‚úì relation: 4 categories ‚Üí 4 dummy variables\n",
      "    Columns: ['relation_Health care professional', 'relation_Parent', 'relation_Relative', 'relation_Self']\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5. DROPPING LEAKAGE AND CONSTANT COLUMNS\n",
      "--------------------------------------------------------------------------------\n",
      "  ‚úì Dropped 'result' (DATA LEAKAGE: sum of A1-A10 scores)\n",
      "  ‚úì Dropped 'age_desc' (CONSTANT: only 1 unique value - no variance)\n",
      "\n",
      "  Total columns dropped: 2\n",
      "\n",
      "================================================================================\n",
      "ENCODING SUMMARY\n",
      "================================================================================\n",
      "  Final shape: (292, 32)\n",
      "  Original columns: 21\n",
      "  Final columns: 32\n",
      "  Columns removed: 0\n",
      "\n",
      "  All columns:\n",
      "    1. A1_Score\n",
      "    2. A2_Score\n",
      "    3. A3_Score\n",
      "    4. A4_Score\n",
      "    5. A5_Score\n",
      "    6. A6_Score\n",
      "    7. A7_Score\n",
      "    8. A8_Score\n",
      "    9. A9_Score\n",
      "    10. A10_Score\n",
      "    11. age\n",
      "    12. gender\n",
      "    13. jaundice\n",
      "    14. autism\n",
      "    15. used_app_before\n",
      "    16. class\n",
      "    17. country_Australia\n",
      "    18. country_India\n",
      "    19. country_Jordan\n",
      "    20. country_Other\n",
      "    21. country_United Kingdom\n",
      "    22. country_United States\n",
      "    23. ethnicity_Asian\n",
      "    24. ethnicity_Black\n",
      "    25. ethnicity_Middle Eastern\n",
      "    26. ethnicity_Others\n",
      "    27. ethnicity_South Asian\n",
      "    28. ethnicity_White-European\n",
      "    29. relation_Health care professional\n",
      "    30. relation_Parent\n",
      "    31. relation_Relative\n",
      "    32. relation_Self\n",
      "\n",
      "  ‚ö†Ô∏è DATA LEAKAGE CHECK:\n",
      "  ‚úÖ SUCCESS: 'result' column removed - no data leakage\n",
      "\n",
      "‚úì Encoded data saved to: d:\\Labs\\ML\\ML Project\\ChildhoodAutismRiskPrediction\\data\\clean\\autism_screening_encoded.csv\n",
      "‚úì Encoding report saved to: d:\\Labs\\ML\\ML Project\\ChildhoodAutismRiskPrediction\\reports\\encoding_report.txt\n",
      "\n",
      "================================================================================\n",
      "ENCODING COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Load the cleaned data\n",
    "df_encoded = pd.read_csv(output_file)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE ENCODING PROCESS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nOriginal shape: {df_encoded.shape}\")\n",
    "\n",
    "# Create encoding report\n",
    "encoding_report = []\n",
    "encoding_report.append(\"=\"*80)\n",
    "encoding_report.append(\"ENCODING REPORT\")\n",
    "encoding_report.append(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1. BINARY ENCODING (2-category columns)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"1. BINARY ENCODING (2-category columns)\")\n",
    "print(\"-\"*80)\n",
    "encoding_report.append(\"\\n1. BINARY ENCODING\")\n",
    "encoding_report.append(\"-\"*80)\n",
    "\n",
    "binary_columns = {\n",
    "    'gender': {'m': 1, 'f': 0},\n",
    "    'jaundice': {'yes': 1, 'no': 0},\n",
    "    'autism': {'yes': 1, 'no': 0},\n",
    "    'used_app_before': {'yes': 1, 'no': 0},\n",
    "    'class': {'YES': 1, 'NO': 0}\n",
    "}\n",
    "\n",
    "for col, mapping in binary_columns.items():\n",
    "    if col in df_encoded.columns:\n",
    "        df_encoded[col] = df_encoded[col].map(mapping)\n",
    "        msg = f\"‚úì {col}: {mapping}\"\n",
    "        print(f\"  {msg}\")\n",
    "        encoding_report.append(f\"  {msg}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. TOP-N + OTHER ENCODING for country_of_res\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"2. TOP-5 COUNTRY ENCODING\")\n",
    "print(\"-\"*80)\n",
    "encoding_report.append(\"\\n\\n2. TOP-5 COUNTRY ENCODING\")\n",
    "encoding_report.append(\"-\"*80)\n",
    "\n",
    "# Get top 5 countries\n",
    "top_5_countries = df_encoded['country_of_res'].value_counts().head(5).index.tolist()\n",
    "print(f\"  Top 5 countries: {top_5_countries}\")\n",
    "encoding_report.append(f\"  Top 5 countries: {top_5_countries}\")\n",
    "\n",
    "# Create a new column with top 5 + Other\n",
    "df_encoded['country_grouped'] = df_encoded['country_of_res'].apply(\n",
    "    lambda x: x if x in top_5_countries else 'Other'\n",
    ")\n",
    "\n",
    "# Show distribution\n",
    "grouped_counts = df_encoded['country_grouped'].value_counts()\n",
    "print(f\"\\n  Grouped distribution:\")\n",
    "encoding_report.append(f\"\\n  Grouped distribution:\")\n",
    "for country, count in grouped_counts.items():\n",
    "    msg = f\"    {country}: {count}\"\n",
    "    print(msg)\n",
    "    encoding_report.append(msg)\n",
    "\n",
    "# One-hot encode the grouped countries\n",
    "country_dummies = pd.get_dummies(df_encoded['country_grouped'], prefix='country', dtype=int)\n",
    "df_encoded = pd.concat([df_encoded, country_dummies], axis=1)\n",
    "\n",
    "# Drop original columns\n",
    "df_encoded.drop(['country_of_res', 'country_grouped'], axis=1, inplace=True)\n",
    "print(f\"\\n  ‚úì Created {len(country_dummies.columns)} country dummy variables\")\n",
    "print(f\"  ‚úì Columns: {list(country_dummies.columns)}\")\n",
    "encoding_report.append(f\"\\n  ‚úì Created {len(country_dummies.columns)} country dummy variables\")\n",
    "encoding_report.append(f\"  ‚úì Columns: {list(country_dummies.columns)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. GROUPED ENCODING for ethnicity (keep top categories with count >= 10)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"3. GROUPED ETHNICITY ENCODING (keep categories with count >= 10)\")\n",
    "print(\"-\"*80)\n",
    "encoding_report.append(\"\\n\\n3. GROUPED ETHNICITY ENCODING\")\n",
    "encoding_report.append(\"-\"*80)\n",
    "\n",
    "if 'ethnicity' in df_encoded.columns:\n",
    "    # Get ethnicity value counts\n",
    "    ethnicity_counts = df_encoded['ethnicity'].value_counts()\n",
    "    print(f\"\\n  Original ethnicity distribution:\")\n",
    "    encoding_report.append(f\"\\n  Original ethnicity distribution:\")\n",
    "    for eth, count in ethnicity_counts.items():\n",
    "        msg = f\"    {eth}: {count}\"\n",
    "        print(msg)\n",
    "        encoding_report.append(msg)\n",
    "    \n",
    "    # Keep categories with count >= 10, merge rest into \"Others\"\n",
    "    top_ethnicities = ethnicity_counts[ethnicity_counts >= 10].index.tolist()\n",
    "    print(f\"\\n  Keeping categories with count >= 10: {top_ethnicities}\")\n",
    "    encoding_report.append(f\"\\n  Keeping categories with count >= 10: {top_ethnicities}\")\n",
    "    \n",
    "    # Group ethnicities\n",
    "    df_encoded['ethnicity_grouped'] = df_encoded['ethnicity'].apply(\n",
    "        lambda x: x if x in top_ethnicities else 'Others'\n",
    "    )\n",
    "    \n",
    "    # Ensure \"Others\" is included in the list if not already\n",
    "    if 'Others' not in top_ethnicities and 'Others' in df_encoded['ethnicity'].values:\n",
    "        # Others is already in the original data, no need to add\n",
    "        pass\n",
    "    \n",
    "    # Show new distribution\n",
    "    grouped_eth_counts = df_encoded['ethnicity_grouped'].value_counts()\n",
    "    print(f\"\\n  Grouped ethnicity distribution:\")\n",
    "    encoding_report.append(f\"\\n  Grouped ethnicity distribution:\")\n",
    "    for eth, count in grouped_eth_counts.items():\n",
    "        msg = f\"    {eth}: {count}\"\n",
    "        print(msg)\n",
    "        encoding_report.append(msg)\n",
    "    \n",
    "    # One-hot encode the grouped ethnicity\n",
    "    n_categories = df_encoded['ethnicity_grouped'].nunique()\n",
    "    eth_dummies = pd.get_dummies(df_encoded['ethnicity_grouped'], prefix='ethnicity', dtype=int)\n",
    "    df_encoded = pd.concat([df_encoded, eth_dummies], axis=1)\n",
    "    df_encoded.drop(['ethnicity', 'ethnicity_grouped'], axis=1, inplace=True)\n",
    "    \n",
    "    msg = f\"‚úì ethnicity: {n_categories} grouped categories ‚Üí {len(eth_dummies.columns)} dummy variables\"\n",
    "    print(f\"\\n  {msg}\")\n",
    "    print(f\"  Columns: {list(eth_dummies.columns)}\")\n",
    "    encoding_report.append(f\"\\n  {msg}\")\n",
    "    encoding_report.append(f\"  Columns: {list(eth_dummies.columns)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. ONE-HOT ENCODING for relation\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"4. ONE-HOT ENCODING (relation column)\")\n",
    "print(\"-\"*80)\n",
    "encoding_report.append(\"\\n\\n4. ONE-HOT ENCODING\")\n",
    "encoding_report.append(\"-\"*80)\n",
    "\n",
    "# First, fix the relation column - remove quotes from 'Health care professional'\n",
    "if 'relation' in df_encoded.columns:\n",
    "    df_encoded['relation'] = df_encoded['relation'].str.strip(\"'\")\n",
    "    print(\"  ‚úì Cleaned 'relation' column (removed quotes)\")\n",
    "\n",
    "# One-hot encode relation\n",
    "if 'relation' in df_encoded.columns:\n",
    "    n_categories = df_encoded['relation'].nunique()\n",
    "    dummies = pd.get_dummies(df_encoded['relation'], prefix='relation', dtype=int)\n",
    "    df_encoded = pd.concat([df_encoded, dummies], axis=1)\n",
    "    df_encoded.drop('relation', axis=1, inplace=True)\n",
    "    \n",
    "    msg = f\"‚úì relation: {n_categories} categories ‚Üí {len(dummies.columns)} dummy variables\"\n",
    "    print(f\"  {msg}\")\n",
    "    print(f\"    Columns: {list(dummies.columns)}\")\n",
    "    encoding_report.append(f\"  {msg}\")\n",
    "    encoding_report.append(f\"    Columns: {list(dummies.columns)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. DROP LEAKAGE AND CONSTANT COLUMNS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"5. DROPPING LEAKAGE AND CONSTANT COLUMNS\")\n",
    "print(\"-\"*80)\n",
    "encoding_report.append(\"\\n\\n5. DROPPING LEAKAGE AND CONSTANT COLUMNS\")\n",
    "encoding_report.append(\"-\"*80)\n",
    "\n",
    "columns_to_drop = []\n",
    "\n",
    "# Drop result column (data leakage - it's sum of A1-A10)\n",
    "if 'result' in df_encoded.columns:\n",
    "    columns_to_drop.append('result')\n",
    "    msg = \"‚úì Dropped 'result' (DATA LEAKAGE: sum of A1-A10 scores)\"\n",
    "    print(f\"  {msg}\")\n",
    "    encoding_report.append(f\"  {msg}\")\n",
    "    encoding_report.append(f\"    Reason: Would give model perfect shortcut, preventing real learning\")\n",
    "\n",
    "# Drop age_desc (constant column)\n",
    "if 'age_desc' in df_encoded.columns:\n",
    "    columns_to_drop.append('age_desc')\n",
    "    msg = \"‚úì Dropped 'age_desc' (CONSTANT: only 1 unique value - no variance)\"\n",
    "    print(f\"  {msg}\")\n",
    "    encoding_report.append(f\"  {msg}\")\n",
    "\n",
    "if columns_to_drop:\n",
    "    df_encoded.drop(columns_to_drop, axis=1, inplace=True)\n",
    "    print(f\"\\n  Total columns dropped: {len(columns_to_drop)}\")\n",
    "    encoding_report.append(f\"\\n  Total columns dropped: {len(columns_to_drop)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENCODING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "encoding_report.append(\"\\n\\n\" + \"=\"*80)\n",
    "encoding_report.append(\"ENCODING SUMMARY\")\n",
    "encoding_report.append(\"=\"*80)\n",
    "\n",
    "print(f\"  Final shape: {df_encoded.shape}\")\n",
    "print(f\"  Original columns: 21\")\n",
    "print(f\"  Final columns: {df_encoded.shape[1]}\")\n",
    "print(f\"  Columns removed: {21 - df_encoded.shape[1] + (df_encoded.shape[1] - 21)}\")\n",
    "\n",
    "encoding_report.append(f\"  Final shape: {df_encoded.shape}\")\n",
    "encoding_report.append(f\"  Original columns: 21\")\n",
    "encoding_report.append(f\"  Final columns: {df_encoded.shape[1]}\")\n",
    "\n",
    "print(f\"\\n  All columns:\")\n",
    "encoding_report.append(f\"\\n  All columns:\")\n",
    "for i, col in enumerate(df_encoded.columns, 1):\n",
    "    print(f\"    {i}. {col}\")\n",
    "    encoding_report.append(f\"    {i}. {col}\")\n",
    "\n",
    "# Verify no data leakage\n",
    "print(f\"\\n  ‚ö†Ô∏è DATA LEAKAGE CHECK:\")\n",
    "encoding_report.append(f\"\\n  ‚ö†Ô∏è DATA LEAKAGE CHECK:\")\n",
    "if 'result' in df_encoded.columns:\n",
    "    warning = \"  ‚ùå WARNING: 'result' column still present - REMOVE IT!\"\n",
    "    print(warning)\n",
    "    encoding_report.append(warning)\n",
    "else:\n",
    "    success = \"  ‚úÖ SUCCESS: 'result' column removed - no data leakage\"\n",
    "    print(success)\n",
    "    encoding_report.append(success)\n",
    "\n",
    "# Save encoded data\n",
    "encoded_output = os.path.join(project_root, 'data', 'clean', 'autism_screening_encoded.csv')\n",
    "df_encoded.to_csv(encoded_output, index=False)\n",
    "print(f\"\\n‚úì Encoded data saved to: {encoded_output}\")\n",
    "\n",
    "# Save encoding report\n",
    "report_path = os.path.join(project_root, 'reports', 'encoding_report.txt')\n",
    "with open(report_path, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(encoding_report))\n",
    "print(f\"‚úì Encoding report saved to: {report_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENCODING COMPLETE!\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b31d22",
   "metadata": {},
   "source": [
    "# Is This the Best Encoding Strategy?\n",
    "\n",
    "## ‚úÖ Yes, this approach is good for most ML models because:\n",
    "\n",
    "### Binary Encoding (0/1):\n",
    "- **Efficient**: Single column per feature\n",
    "- **Works with**: Linear models, tree-based models, neural networks\n",
    "- **Best for**: Features with natural binary states (yes/no, male/female)\n",
    "\n",
    "### One-Hot Encoding:\n",
    "- **Prevents false ordinal relationships**: Each category is independent\n",
    "- **Works with**: Linear models (Logistic Regression, Linear SVM), Neural Networks\n",
    "- **Already optimized for**: Tree-based models (Random Forest, XGBoost, etc.) can handle categorical data directly, but one-hot encoding still works well\n",
    "\n",
    "### Top-N + Other Strategy:\n",
    "- **Reduces dimensionality**: 52 columns ‚Üí 6 columns (manageable)\n",
    "- **Prevents overfitting**: Too many sparse categories can hurt model performance\n",
    "- **Captures majority patterns**: Top 5 countries represent ~60% of data\n",
    "\n",
    "## üìä Model Considerations:\n",
    "\n",
    "**Tree-based models** (Random Forest, XGBoost, CatBoost):\n",
    "- ‚úÖ Work excellently with this encoding\n",
    "- ‚ÑπÔ∏è CatBoost can handle categorical features natively, but this encoding still works\n",
    "\n",
    "**Linear models** (Logistic Regression, SVM):\n",
    "- ‚úÖ Require this type of encoding\n",
    "- ‚ö†Ô∏è May need feature scaling for numeric columns (age, result)\n",
    "\n",
    "**Neural Networks**:\n",
    "- ‚úÖ Work well with this encoding\n",
    "- ‚ö†Ô∏è Definitely need feature scaling\n",
    "\n",
    "## üí° Alternative Approach:\n",
    "If using **CatBoost**, you could keep categorical columns as-is and let it handle them natively. But your current approach is **universal and production-ready** for all ML models!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
